server:
  host: "0.0.0.0"
  port: 8000
  workers: 8          # 4 → 8 (동시접속 증가)
  log_level: "info"

# Cloudflare Tunnel
cloudflare:
  enabled: false  # 나중에 설정
  tunnel_name: "llm-api"
  # 도메인은 Cloudflare 대시보드에서 설정

# OpenAI (ChatGPT)
openai:
  api_key: ""  # .env 파일에서 OPENAI_API_KEY로 설정
  enabled: true
  timeout: 120  # 초

# 기본 모델 (vLLM-MLX 사용)
default_model: "vllm-qwen3-30b-a3b"

# 사용 가능 모델 (UI에서 선택 가능)
available_models:
  # === vLLM-MLX 모델 (Continuous Batching - 추천!) ===
  - name: "vllm-qwen3-30b-a3b"
    display_name: "Qwen3 30B MoE (217 t/s, 동시6명) ⚡⚡"
    context_length: 40960
    provider: "vllm-mlx"
    recommended_for: "최고 속도 + 동시 처리 (추천)"

  # === OpenAI GPT 모델 (클라우드 백업) ===
  - name: "gpt-5"
    display_name: "GPT-5 (최신)"
    context_length: 128000
    provider: "openai"
    recommended_for: "최고 성능 OpenAI 모델"

  - name: "gpt-5-mini"
    display_name: "GPT-5 Mini (빠름)"
    context_length: 128000
    provider: "openai"
    recommended_for: "클라우드 백업용"

# 인증
auth:
  enabled: false  # 비활성화 - 누구나 사용 가능
  # Admin key는 환경변수 ADMIN_API_KEY에서 로드

# Rate Limiting
rate_limit:
  enabled: true
  requests_per_minute: 60
  tokens_per_minute: 100000

# CORS (Cloudflare Tunnel 사용시 도메인 추가)
cors:
  allowed_origins:
    - "*"  # 또는 특정 도메인만

# 로드밸런싱 설정
load_balancing:
  enabled: true
  auto_fallback: true  # 자동 클라우드 전환
  prefer_local: true   # 로컬 우선 사용

  # 로컬 모델 (기본)
  local_model: "vllm-qwen3-30b-a3b"

  # 클라우드 모델 (7명 이상시 전환)
  cloud_model: "gpt-5-mini"

  # 임계값 설정 (1~6명 로컬 보장, 7명부터 클라우드)
  max_queue_size: 6      # 로컬 최대 동시 처리 (4→6명)
  max_wait_time: 5.0     # 최대 대기 시간 (3→5초)

# 성능 최적화 설정
optimization:
  default_max_tokens: 4096    # 기본 최대 토큰 (메모리 절약)
  stream_chunk_size: 20       # 스트리밍 청크 크기 (10→20)
  response_timeout: 180       # 응답 타임아웃 (120→180초)
  preload_model: true         # 서버 시작 시 모델 프리로드

# vLLM-MLX 설정 (Continuous Batching - 추천!)
vllm_mlx:
  enabled: true
  base_url: "http://localhost:8001"
  model: "mlx-community/Qwen3-30B-A3B-4bit"
  max_tokens: 4096
  temperature: 0.7
  timeout: 180         # 120 → 180 (긴 응답 허용)

# MLX 설정 (단일 사용자용 - 백업)
mlx:
  enabled: false  # vLLM-MLX 사용 시 비활성화
  model_path: "mlx-community/Qwen3-30B-A3B-4bit"
  max_tokens: 4096
  temperature: 0.7
  preload: false
