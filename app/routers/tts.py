"""TTS (Text-to-Speech) API ë¼ìš°í„° - CosyVoice 3.0 ê¸°ë°˜"""
import os
import sys
import io
import tempfile
from typing import Optional
from fastapi import APIRouter, HTTPException
from fastapi.responses import Response
from pydantic import BaseModel

# CosyVoice ê²½ë¡œ ì¶”ê°€
COSYVOICE_PATH = '/Users/mindprep/CosyVoice'
sys.path.insert(0, COSYVOICE_PATH)
sys.path.insert(0, f'{COSYVOICE_PATH}/third_party/Matcha-TTS')

router = APIRouter(prefix="/tts", tags=["TTS"])

# ì „ì—­ CosyVoice ëª¨ë¸ (ì‹±ê¸€í†¤)
_cosyvoice_model = None
_is_loading = False

# ì°¸ì¡° ìŒì„± ì„¤ì •
REFERENCE_AUDIO = f'{COSYVOICE_PATH}/reference_audio.wav'
PROMPT_TEXT = "ì•ˆë…•í•˜ì„¸ìš”! ë©´ì ‘ì— ì°¸ì—¬í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ë¨¼ì €, ë³¸ì¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ì†Œê°œí•´ ì£¼ì‹œê² ì–´ìš”?"


class TTSRequest(BaseModel):
    """TTS ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    text: str
    speed: float = 1.0


class TTSStatus(BaseModel):
    """TTS ìƒíƒœ ì‘ë‹µ"""
    available: bool
    model_loaded: bool
    device: str
    sample_rate: Optional[int] = None
    reference_audio: Optional[str] = None


def get_cosyvoice_model():
    """CosyVoice ëª¨ë¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _cosyvoice_model, _is_loading

    if _cosyvoice_model is not None:
        return _cosyvoice_model

    if _is_loading:
        return None

    _is_loading = True

    try:
        import torch
        from cosyvoice.cli.cosyvoice import CosyVoice3

        # ë””ë°”ì´ìŠ¤ í™•ì¸
        if torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"

        print(f"ğŸ¤ CosyVoice ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {device})")

        # ëª¨ë¸ ê²½ë¡œ (models í´ë” í¬í•¨)
        model_path = '/Users/mindprep/.cache/modelscope/hub/models/FunAudioLLM/Fun-CosyVoice3-0___5B-2512'

        if os.path.exists(model_path):
            _cosyvoice_model = CosyVoice3(model_path)
        else:
            print("CosyVoice ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            _cosyvoice_model = CosyVoice3('FunAudioLLM/Fun-CosyVoice3-0.5B-2512')

        print(f"âœ… CosyVoice ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ìƒ˜í”Œë ˆì´íŠ¸: {_cosyvoice_model.sample_rate})")
        return _cosyvoice_model

    except Exception as e:
        print(f"âŒ CosyVoice ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        _is_loading = False
        return None

    finally:
        _is_loading = False


@router.get("/status", response_model=TTSStatus)
async def get_tts_status():
    """TTS ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    import torch

    model = get_cosyvoice_model()

    if torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"

    if model is not None:
        return TTSStatus(
            available=True,
            model_loaded=True,
            device=device,
            sample_rate=model.sample_rate,
            reference_audio=REFERENCE_AUDIO if os.path.exists(REFERENCE_AUDIO) else None
        )
    else:
        return TTSStatus(
            available=False,
            model_loaded=False,
            device=device,
            sample_rate=None,
            reference_audio=None
        )


@router.post("/synthesize")
async def synthesize_speech(request: TTSRequest):
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜ (Zero-shot ìŒì„± í´ë¡œë‹)"""
    import soundfile as sf

    model = get_cosyvoice_model()

    if model is None:
        raise HTTPException(
            status_code=503,
            detail="TTS ëª¨ë¸ì´ ë¡œë”© ì¤‘ì´ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )

    if not os.path.exists(REFERENCE_AUDIO):
        raise HTTPException(
            status_code=500,
            detail=f"ì°¸ì¡° ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {REFERENCE_AUDIO}"
        )

    text = request.text.strip()
    if not text:
        raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    try:
        # ì´ì „ í…ŒìŠ¤íŠ¸ ë°©ì‹ (í•œêµ­ì–´ ì‘ë™ í™•ì¸ë¨)
        # tts_textì™€ prompt_text ëª¨ë‘ <|endofprompt|> í† í° ì•ì— ë¶™ì„
        tts_text = f"<|endofprompt|>{text}"
        prompt_with_token = f"<|endofprompt|>{PROMPT_TEXT}"

        # ìŒì„± í•©ì„±
        audio_data = None
        for result in model.inference_zero_shot(
            tts_text=tts_text,
            prompt_text=prompt_with_token,
            prompt_wav=REFERENCE_AUDIO,
            stream=False,
            speed=request.speed,
            text_frontend=True  # í…ìŠ¤íŠ¸ ì •ê·œí™” í™œì„±í™”
        ):
            audio_data = result['tts_speech']
            break  # ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš©

        if audio_data is None:
            raise HTTPException(status_code=500, detail="ìŒì„± í•©ì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # WAV íŒŒì¼ë¡œ ë³€í™˜
        audio_np = audio_data.squeeze().cpu().numpy()

        # ë©”ëª¨ë¦¬ì—ì„œ WAV ìƒì„±
        buffer = io.BytesIO()
        sf.write(buffer, audio_np, model.sample_rate, format='WAV')
        buffer.seek(0)

        return Response(
            content=buffer.read(),
            media_type="audio/wav",
            headers={
                "Content-Disposition": "attachment; filename=tts_output.wav"
            }
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ìŒì„± í•©ì„± ì˜¤ë¥˜: {str(e)}"
        )


@router.post("/warmup")
async def warmup_tts():
    """TTS ëª¨ë¸ ì›œì—… (ì‚¬ì „ ë¡œë”©)"""
    model = get_cosyvoice_model()

    if model is not None:
        return {
            "status": "ok",
            "message": "TTS ëª¨ë¸ ë¡œë”© ì™„ë£Œ",
            "sample_rate": model.sample_rate
        }
    else:
        return {
            "status": "loading",
            "message": "TTS ëª¨ë¸ ë¡œë”© ì¤‘..."
        }
