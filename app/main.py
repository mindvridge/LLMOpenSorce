"""FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
import os
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from app.config import get_config, get_settings
from app.routers import chat, models, health
from app.ollama_client import get_ollama_client


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    print("ğŸš€ LLM API Server ì‹œì‘ ì¤‘...")

    # ì„¤ì • ë¡œë“œ
    config = get_config()
    settings = get_settings()

    print(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    print(f"   - ê¸°ë³¸ ëª¨ë¸: {config.default_model}")
    print(f"   - Ollama URL: {config.ollama.base_url}")

    # Ollama ì—°ê²° í™•ì¸
    ollama_client = get_ollama_client()
    is_connected = await ollama_client.health_check()

    if is_connected:
        print(f"âœ… Ollama ì—°ê²° ì„±ê³µ")

        # ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
        try:
            installed_models = await ollama_client.list_models()
            print(f"âœ… ì„¤ì¹˜ëœ ëª¨ë¸: {len(installed_models)}ê°œ")
            for model in installed_models:
                model_name = model.get("name", "unknown")
                model_size = model.get("size", 0)
                size_gb = model_size / (1024 ** 3) if model_size else 0
                print(f"   - {model_name} ({size_gb:.1f} GB)")
        except Exception as e:
            print(f"âš ï¸  ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    else:
        print(f"âš ï¸  Ollama ì—°ê²° ì‹¤íŒ¨ - ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")

    print(f"\nğŸ“¡ ì„œë²„ ì‹¤í–‰ ì¤‘:")
    print(f"   - Local: http://localhost:{settings.SERVER_PORT}")
    print(f"   - Health: http://localhost:{settings.SERVER_PORT}/health")
    print(f"   - Docs: http://localhost:{settings.SERVER_PORT}/docs")

    yield

    # ì¢…ë£Œ ì‹œ
    print("\nğŸ›‘ LLM API Server ì¢…ë£Œ ì¤‘...")
    await ollama_client.close()
    print("âœ… í´ë¦°ì—… ì™„ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LLM API Server",
    description="Ollama ê¸°ë°˜ OpenAI í˜¸í™˜ LLM API ì„œë²„",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS ì„¤ì •
config = get_config()
app.add_middleware(
    CORSMiddleware,
    allow_origins=config.cors.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": {
                "message": f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(exc)}",
                "type": "internal_error",
                "code": "internal_error",
            }
        },
    )


# ë¼ìš°í„° ë“±ë¡
app.include_router(health.router, tags=["Health"])
app.include_router(chat.router, tags=["Chat"])
app.include_router(models.router, tags=["Models"])


# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """API ë£¨íŠ¸"""
    return {
        "name": "LLM API Server",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "chat": "/v1/chat/completions",
            "models": "/v1/models",
            "docs": "/docs",
        },
    }


if __name__ == "__main__":
    import uvicorn

    settings = get_settings()
    uvicorn.run(
        "app.main:app",
        host=settings.SERVER_HOST,
        port=settings.SERVER_PORT,
        reload=True,
        log_level="info",
    )
