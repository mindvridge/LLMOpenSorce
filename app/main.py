"""FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
import sys
# MLX íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€ (Apple Silicon ìµœì í™”)
sys.path.insert(0, '/Users/mindprep/Library/Python/3.9/lib/python/site-packages')

import os
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from app.config import get_config, get_settings
from app.routers import chat, models, health, admin, monitor, resume, prompts, rag, tts
from app.clients.openai_client import get_openai_client
from app.clients.mlx_client import get_mlx_client
from app.database import init_db
from app.load_balancer import get_load_balancer, init_load_balancer, LoadBalancerConfig


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    print("ğŸš€ LLM API Server ì‹œì‘ ì¤‘...")

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    init_db()
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    # ì„¤ì • ë¡œë“œ
    config = get_config()
    settings = get_settings()

    print(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    print(f"   - ê¸°ë³¸ ëª¨ë¸: {config.default_model}")
    print(f"   - ì¸ì¦ í™œì„±í™”: {config.auth.enabled}")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_client = get_openai_client()
    if openai_client.is_enabled():
        print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ í™œì„±í™”")
    else:
        print(f"âš ï¸  OpenAI í´ë¼ì´ì–¸íŠ¸ ë¹„í™œì„±í™” (OPENAI_API_KEY ë¯¸ì„¤ì •)")

    # ë¡œë“œë°¸ëŸ°ì„œ ì´ˆê¸°í™” (config.yaml ì„¤ì • ì‚¬ìš©)
    lb_settings = config.load_balancing
    lb_config = LoadBalancerConfig(
        enabled=lb_settings.enabled,
        local_model=lb_settings.local_model,
        cloud_model=lb_settings.cloud_model,
        max_queue_size=lb_settings.max_queue_size,
        max_wait_time=lb_settings.max_wait_time,
        auto_fallback=lb_settings.auto_fallback,
        prefer_local=lb_settings.prefer_local
    )
    init_load_balancer(lb_config)
    print(f"âœ… ë¡œë“œë°¸ëŸ°ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
    print(f"   - ë¡œì»¬ ëª¨ë¸: {lb_config.local_model}")
    print(f"   - í´ë¼ìš°ë“œ ëª¨ë¸: {lb_config.cloud_model}")
    print(f"   - ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {lb_config.max_queue_size}ëª… (ì´ˆê³¼ ì‹œ í´ë¼ìš°ë“œ)")

    # vLLM-MLX ì›œì—… (Continuous Batching ì„œë²„)
    print("\nğŸ”¥ vLLM-MLX ì›œì—… ì¤‘...")
    try:
        import httpx
        async with httpx.AsyncClient(timeout=120.0) as client:
            # vLLM-MLX ì„œë²„ í™•ì¸
            health_resp = await client.get("http://localhost:8001/v1/models")
            if health_resp.status_code == 200:
                print(f"   - vLLM-MLX ì„œë²„ ì—°ê²° ì„±ê³µ")

                # ì›œì—… ì¶”ë¡  ì‹¤í–‰ (ì²« ì¶”ë¡  ì§€ì—° ì œê±°)
                print(f"   - ì›œì—… ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
                warmup_resp = await client.post(
                    "http://localhost:8001/v1/chat/completions",
                    json={
                        "model": "mlx-community/Qwen3-30B-A3B-4bit",
                        "messages": [{"role": "user", "content": "Hi /nothink"}],
                        "max_tokens": 10
                    }
                )
                if warmup_resp.status_code == 200:
                    print(f"âœ… vLLM-MLX ì›œì—… ì™„ë£Œ")
                else:
                    print(f"âš ï¸  vLLM-MLX ì›œì—… ì‘ë‹µ ì˜¤ë¥˜: {warmup_resp.status_code}")
            else:
                print(f"âš ï¸  vLLM-MLX ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
    except Exception as e:
        print(f"âš ï¸  vLLM-MLX ì›œì—… ì‹¤íŒ¨: {str(e)}")

    # RAG ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ì €ì¥ì†Œ ì›œì—…
    print("\nğŸ“š RAG ì‹œìŠ¤í…œ ì›œì—… ì¤‘...")
    try:
        from app.rag.embeddings import get_embedding_client
        from app.rag.vector_store import get_vector_store

        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
        print("   - ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘ (jhgan/ko-sroberta-multitask)...")
        embedding_client = get_embedding_client()
        _ = embedding_client.embed_query("ì›œì—… í…ŒìŠ¤íŠ¸")
        print("   âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # 2. ChromaDB ì´ˆê¸°í™”
        print("   - ChromaDB ì´ˆê¸°í™” ì¤‘...")
        vector_store = get_vector_store()
        _ = vector_store.list_collections()
        print("   âœ… ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")

        print("âœ… RAG ì‹œìŠ¤í…œ ì›œì—… ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸  RAG ì›œì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {str(e)}")

    # ì§ˆë¬¸ì…‹ ë¡œë“œ ë° ì¸ë±ì‹±
    print("\nğŸ“‹ ì§ˆë¬¸ì…‹ ë¡œë“œ ì¤‘...")
    try:
        from app.question_sets import load_all_question_sets, index_all_question_sets
        load_all_question_sets()

        # ì§ˆë¬¸ì…‹ ChromaDB ì¸ë±ì‹±
        print("\nğŸ” ì§ˆë¬¸ì…‹ RAG ì¸ë±ì‹± ì¤‘...")
        index_all_question_sets()
    except Exception as e:
        print(f"âš ï¸  ì§ˆë¬¸ì…‹ ë¡œë“œ/ì¸ë±ì‹± ì‹¤íŒ¨ (ë¬´ì‹œë¨): {str(e)}")

    # TTS (CosyVoice) ëª¨ë¸ ì›œì—…
    print("\nğŸ¤ TTS ëª¨ë¸ ì›œì—… ì¤‘...")
    try:
        from app.routers.tts import get_cosyvoice_model
        tts_model = get_cosyvoice_model()
        if tts_model is not None:
            print(f"âœ… CosyVoice TTS ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ìƒ˜í”Œë ˆì´íŠ¸: {tts_model.sample_rate})")
        else:
            print(f"âš ï¸  TTS ëª¨ë¸ ë¡œë”© ì¤‘... (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì†)")
    except Exception as e:
        print(f"âš ï¸  TTS ì›œì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {str(e)}")

    print(f"\nğŸ“¡ ì„œë²„ ì‹¤í–‰ ì¤‘:")
    print(f"   - Local: http://localhost:{settings.SERVER_PORT}")
    print(f"   - Health: http://localhost:{settings.SERVER_PORT}/health")
    print(f"   - Docs: http://localhost:{settings.SERVER_PORT}/docs")

    yield

    # ì¢…ë£Œ ì‹œ
    print("\nğŸ›‘ LLM API Server ì¢…ë£Œ ì¤‘...")
    print("âœ… í´ë¦°ì—… ì™„ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LLM API Server",
    description="vLLM-MLX ë° OpenAI ê¸°ë°˜ LLM API ì„œë²„",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS ì„¤ì •
config = get_config()
app.add_middleware(
    CORSMiddleware,
    allow_origins=config.cors.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": {
                "message": f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(exc)}",
                "type": "internal_error",
                "code": "internal_error",
            }
        },
    )


# ë¼ìš°í„° ë“±ë¡
app.include_router(health.router, tags=["Health"])
app.include_router(chat.router, tags=["Chat"])
app.include_router(models.router, tags=["Models"])
app.include_router(admin.router, tags=["Admin"])
app.include_router(monitor.router, tags=["Monitoring"])
app.include_router(resume.router, tags=["Resume"])
app.include_router(prompts.router, tags=["Prompts"])
app.include_router(rag.router, tags=["RAG"])
app.include_router(tts.router, tags=["TTS"])


# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """API ë£¨íŠ¸"""
    return {
        "name": "LLM API Server",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "chat_api": "/v1/chat/completions",
            "models": "/v1/models",
            "docs": "/docs",
            "test": "/test",
            "chat": "/chat",
            "chat_streaming": "/chat-streaming",
            "dashboard": "/dashboard",
        },
    }


# ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard_page():
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
    try:
        with open("dashboard.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ëŒ€ì‹œë³´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# í…ŒìŠ¤íŠ¸ í˜ì´ì§€ ì—”ë“œí¬ì¸íŠ¸
@app.get("/test", response_class=HTMLResponse)
async def test_page():
    """í…ŒìŠ¤íŠ¸ í˜ì´ì§€"""
    try:
        with open("test_page.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>í…ŒìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# ì±„íŒ… UI í˜ì´ì§€
@app.get("/chat", response_class=HTMLResponse)
async def chat_page():
    """ì±„íŒ… UI í˜ì´ì§€"""
    try:
        with open("chat_ui.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ì±„íŒ… í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… UI í˜ì´ì§€
@app.get("/chat-streaming", response_class=HTMLResponse)
async def chat_streaming_page():
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… UI í˜ì´ì§€"""
    try:
        with open("chat_ui_streaming.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# ë¡œë“œë°¸ëŸ°ì„œ ìƒíƒœ ì¡°íšŒ
@app.get("/lb/status")
async def load_balancer_status():
    """ë¡œë“œë°¸ëŸ°ì„œ ìƒíƒœ ì¡°íšŒ"""
    lb = get_load_balancer()
    return lb.get_status()


if __name__ == "__main__":
    import uvicorn

    settings = get_settings()
    uvicorn.run(
        "app.main:app",
        host=settings.SERVER_HOST,
        port=settings.SERVER_PORT,
        reload=True,
        log_level="info",
    )
