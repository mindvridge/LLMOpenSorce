"""FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
import sys
# MLX íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€ (Apple Silicon ìµœì í™”)
sys.path.insert(0, '/Users/mindprep/Library/Python/3.9/lib/python/site-packages')

import os
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from app.config import get_config, get_settings, is_railway_environment
from app.routers import chat, models, health, admin, monitor, resume, prompts, rag
from app.clients.openai_client import get_openai_client
from app.clients.mlx_client import get_mlx_client
from app.database import init_db
from app.load_balancer import get_load_balancer, init_load_balancer, LoadBalancerConfig


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    print("ğŸš€ LLM API Server ì‹œì‘ ì¤‘...")

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    init_db()
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    # ì„¤ì • ë¡œë“œ
    config = get_config()
    settings = get_settings()

    print(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    print(f"   - ê¸°ë³¸ ëª¨ë¸: {config.default_model}")
    print(f"   - ì¸ì¦ í™œì„±í™”: {config.auth.enabled}")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_client = get_openai_client()
    if openai_client.is_enabled():
        print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ í™œì„±í™”")
    else:
        print(f"âš ï¸  OpenAI í´ë¼ì´ì–¸íŠ¸ ë¹„í™œì„±í™” (OPENAI_API_KEY ë¯¸ì„¤ì •)")

    # ë¡œë“œë°¸ëŸ°ì„œ ì´ˆê¸°í™” (config.yaml ì„¤ì • ì‚¬ìš©)
    lb_settings = config.load_balancing
    lb_config = LoadBalancerConfig(
        enabled=lb_settings.enabled,
        local_model=lb_settings.local_model,
        cloud_model=lb_settings.cloud_model,
        max_queue_size=lb_settings.max_queue_size,
        max_wait_time=lb_settings.max_wait_time,
        auto_fallback=lb_settings.auto_fallback,
        prefer_local=lb_settings.prefer_local
    )
    init_load_balancer(lb_config)
    print(f"âœ… ë¡œë“œë°¸ëŸ°ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
    print(f"   - ë¡œì»¬ ëª¨ë¸: {lb_config.local_model}")
    print(f"   - í´ë¼ìš°ë“œ ëª¨ë¸: {lb_config.cloud_model}")
    print(f"   - ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {lb_config.max_queue_size}ëª… (ì´ˆê³¼ ì‹œ í´ë¼ìš°ë“œ)")

    # ===== ë³‘ë ¬ ì›œì—… í•¨ìˆ˜ ì •ì˜ =====
    async def warmup_vllm():
        """vLLM-MLX ì›œì—… (Continuous Batching ì„œë²„)"""
        try:
            import httpx
            async with httpx.AsyncClient(timeout=120.0) as client:
                health_resp = await client.get("http://localhost:8001/v1/models")
                if health_resp.status_code == 200:
                    warmup_resp = await client.post(
                        "http://localhost:8001/v1/chat/completions",
                        json={
                            "model": "mlx-community/Qwen3-30B-A3B-4bit",
                            "messages": [{"role": "user", "content": "Hi /nothink"}],
                            "max_tokens": 10
                        }
                    )
                    if warmup_resp.status_code == 200:
                        return "âœ… vLLM-MLX ì›œì—… ì™„ë£Œ"
                    return f"âš ï¸  vLLM-MLX ì›œì—… ì‘ë‹µ ì˜¤ë¥˜: {warmup_resp.status_code}"
                return "âš ï¸  vLLM-MLX ì„œë²„ ì—°ê²° ì‹¤íŒ¨"
        except Exception as e:
            return f"âš ï¸  vLLM-MLX ì›œì—… ì‹¤íŒ¨: {str(e)}"

    async def warmup_rag():
        """RAG ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ì €ì¥ì†Œ ì›œì—…"""
        try:
            from app.rag.embeddings import get_embedding_client
            from app.rag.vector_store import get_vector_store

            # ì„ë² ë”© ëª¨ë¸ ë¡œë”© (ë™ê¸° ì‘ì—…ì„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
            embedding_client = await asyncio.to_thread(get_embedding_client)
            await asyncio.to_thread(embedding_client.embed_query, "ì›œì—… í…ŒìŠ¤íŠ¸")

            # ChromaDB ì´ˆê¸°í™”
            vector_store = await asyncio.to_thread(get_vector_store)
            await asyncio.to_thread(vector_store.list_collections)

            return "âœ… RAG ì‹œìŠ¤í…œ ì›œì—… ì™„ë£Œ"
        except Exception as e:
            return f"âš ï¸  RAG ì›œì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {str(e)}"

    async def warmup_question_sets():
        """ì§ˆë¬¸ì…‹ ë¡œë“œ ë° ì¸ë±ì‹±"""
        try:
            from app.question_sets import load_all_question_sets, index_all_question_sets

            await asyncio.to_thread(load_all_question_sets)
            await asyncio.to_thread(index_all_question_sets)

            return "âœ… ì§ˆë¬¸ì…‹ ë¡œë“œ/ì¸ë±ì‹± ì™„ë£Œ"
        except Exception as e:
            return f"âš ï¸  ì§ˆë¬¸ì…‹ ë¡œë“œ/ì¸ë±ì‹± ì‹¤íŒ¨ (ë¬´ì‹œë¨): {str(e)}"

    # ===== ë³‘ë ¬ ì›œì—… ì‹¤í–‰ (í™˜ê²½ì— ë”°ë¼ ë‹¤ë¦„) =====
    is_railway = is_railway_environment()

    if is_railway:
        # Railway í™˜ê²½: vLLM-MLX ê±´ë„ˆë›°ê¸° (í´ë¼ìš°ë“œ APIë§Œ ì‚¬ìš©)
        print("\nğŸ”¥ ë³‘ë ¬ ì›œì—… ì‹œì‘ (RAG, ì§ˆë¬¸ì…‹) - Railway ëª¨ë“œ")
        warmup_results = await asyncio.gather(
            warmup_rag(),
            warmup_question_sets(),
            return_exceptions=True
        )
        task_names = ["RAG ì‹œìŠ¤í…œ", "ì§ˆë¬¸ì…‹"]
    else:
        # ë¡œì»¬ í™˜ê²½: ì „ì²´ ì›œì—…
        print("\nğŸ”¥ ë³‘ë ¬ ì›œì—… ì‹œì‘ (vLLM-MLX, RAG, ì§ˆë¬¸ì…‹) - ë¡œì»¬ ëª¨ë“œ")
        warmup_results = await asyncio.gather(
            warmup_vllm(),
            warmup_rag(),
            warmup_question_sets(),
            return_exceptions=True
        )
        task_names = ["vLLM-MLX", "RAG ì‹œìŠ¤í…œ", "ì§ˆë¬¸ì…‹"]

    # ì›œì—… ê²°ê³¼ ì¶œë ¥
    for name, result in zip(task_names, warmup_results):
        if isinstance(result, Exception):
            print(f"   âš ï¸  {name} ì›œì—… ì˜ˆì™¸: {str(result)}")
        else:
            print(f"   {result}")


    print(f"\nğŸ“¡ ì„œë²„ ì‹¤í–‰ ì¤‘:")
    print(f"   - Local: http://localhost:{settings.SERVER_PORT}")
    print(f"   - Health: http://localhost:{settings.SERVER_PORT}/health")
    print(f"   - Docs: http://localhost:{settings.SERVER_PORT}/docs")

    yield

    # ì¢…ë£Œ ì‹œ
    print("\nğŸ›‘ LLM API Server ì¢…ë£Œ ì¤‘...")
    print("âœ… í´ë¦°ì—… ì™„ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LLM API Server",
    description="vLLM-MLX ë° OpenAI ê¸°ë°˜ LLM API ì„œë²„",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS ì„¤ì •
config = get_config()
app.add_middleware(
    CORSMiddleware,
    allow_origins=config.cors.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": {
                "message": f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(exc)}",
                "type": "internal_error",
                "code": "internal_error",
            }
        },
    )


# ë¼ìš°í„° ë“±ë¡
app.include_router(health.router, tags=["Health"])
app.include_router(chat.router, tags=["Chat"])
app.include_router(models.router, tags=["Models"])
app.include_router(admin.router, tags=["Admin"])
app.include_router(monitor.router, tags=["Monitoring"])
app.include_router(resume.router, tags=["Resume"])
app.include_router(prompts.router, tags=["Prompts"])
app.include_router(rag.router, tags=["RAG"])


# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """API ë£¨íŠ¸"""
    return {
        "name": "LLM API Server",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "chat_api": "/v1/chat/completions",
            "models": "/v1/models",
            "docs": "/docs",
            "test": "/test",
            "chat": "/chat",
            "chat_streaming": "/chat-streaming",
            "dashboard": "/dashboard",
        },
    }


# ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard_page():
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
    try:
        with open("dashboard.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ëŒ€ì‹œë³´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# í…ŒìŠ¤íŠ¸ í˜ì´ì§€ ì—”ë“œí¬ì¸íŠ¸
@app.get("/test", response_class=HTMLResponse)
async def test_page():
    """í…ŒìŠ¤íŠ¸ í˜ì´ì§€"""
    try:
        with open("test_page.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>í…ŒìŠ¤íŠ¸ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# ì±„íŒ… UI í˜ì´ì§€
@app.get("/chat", response_class=HTMLResponse)
async def chat_page():
    """ì±„íŒ… UI í˜ì´ì§€"""
    try:
        with open("chat_ui.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ì±„íŒ… í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… UI í˜ì´ì§€
@app.get("/chat-streaming", response_class=HTMLResponse)
async def chat_streaming_page():
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… UI í˜ì´ì§€"""
    try:
        with open("chat_ui_streaming.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</h1>",
            status_code=404
        )


# ë¡œë“œë°¸ëŸ°ì„œ ìƒíƒœ ì¡°íšŒ
@app.get("/lb/status")
async def load_balancer_status():
    """ë¡œë“œë°¸ëŸ°ì„œ ìƒíƒœ ì¡°íšŒ"""
    lb = get_load_balancer()
    return lb.get_status()


if __name__ == "__main__":
    import uvicorn

    settings = get_settings()
    uvicorn.run(
        "app.main:app",
        host=settings.SERVER_HOST,
        port=settings.SERVER_PORT,
        reload=True,
        log_level="info",
    )
