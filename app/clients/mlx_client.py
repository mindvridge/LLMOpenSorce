"""MLX LLM í´ë¼ì´ì–¸íŠ¸ - Apple Silicon ìµœì í™” (ë©€í‹° ëª¨ë¸ ì§€ì›)"""
import asyncio
from typing import AsyncGenerator, Optional, Dict, Any, List, Union
from dataclasses import dataclass
import time


# MLX ëª¨ë¸ ë§¤í•‘ (API ëª¨ë¸ëª… -> ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” HuggingFace ê²½ë¡œ)
import os

# ë¡œì»¬ ìºì‹œ ê²½ë¡œ
MLX_CACHE_DIR = os.path.expanduser("~/.cache/mlx-models")

MLX_MODEL_MAP = {
    "mlx-qwen2.5-14b": {
        "hf_path": "mlx-community/Qwen2.5-14B-Instruct-4bit",
        "local_path": os.path.join(MLX_CACHE_DIR, "Qwen2.5-14B-Instruct-4bit"),
    },
    "mlx-qwen3-32b": {
        "hf_path": "mlx-community/Qwen3-32B-4bit",
        "local_path": os.path.join(MLX_CACHE_DIR, "Qwen3-32B-4bit"),
    },
    # Qwen3-30B-A3B: MoE ëª¨ë¸ (30B íŒŒë¼ë¯¸í„°, 3B í™œì„±í™”) - 5ë°° ë¹ ë¦„!
    "mlx-qwen3-30b-a3b": {
        "hf_path": "mlx-community/Qwen3-30B-A3B-4bit",
        "local_path": os.path.join(MLX_CACHE_DIR, "Qwen3-30B-A3B-4bit"),
    },
}


def get_model_path(model_name: str) -> str:
    """ëª¨ë¸ ê²½ë¡œ ë°˜í™˜ (ë¡œì»¬ ìš°ì„ )"""
    model_info = MLX_MODEL_MAP.get(model_name)
    if not model_info:
        return None
    # ë¡œì»¬ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if os.path.exists(model_info["local_path"]):
        return model_info["local_path"]
    # ì—†ìœ¼ë©´ HuggingFace ê²½ë¡œ ë°˜í™˜
    return model_info["hf_path"]


import re

def remove_think_tags(text: str) -> str:
    """Qwen3ì˜ <think>...</think> íƒœê·¸ ì œê±°"""
    # <think>...</think> ë¸”ë¡ ì „ì²´ ì œê±°
    cleaned = re.sub(r'<think>.*?</think>\s*', '', text, flags=re.DOTALL)
    # ë¶ˆì™„ì „í•œ <think> íƒœê·¸ë„ ì œê±° (ì‘ë‹µì´ ì¤‘ê°„ì— ì˜ë¦° ê²½ìš°)
    cleaned = re.sub(r'<think>.*$', '', cleaned, flags=re.DOTALL)
    return cleaned.strip()


@dataclass
class MLXConfig:
    """MLX í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
    default_model: str = "mlx-qwen2.5-14b"
    max_tokens: int = 4096
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.1


class MLXClient:
    """MLX ê¸°ë°˜ LLM í´ë¼ì´ì–¸íŠ¸ (ë©€í‹° ëª¨ë¸ ì§€ì›)"""

    def __init__(self, config: Optional[MLXConfig] = None):
        self.config = config or MLXConfig()
        self._models: Dict[str, Any] = {}  # ëª¨ë¸ ìºì‹œ
        self._tokenizers: Dict[str, Any] = {}  # í† í¬ë‚˜ì´ì € ìºì‹œ
        self._current_model: Optional[str] = None
        self._load_lock = asyncio.Lock()
        # MLXëŠ” ë™ì‹œ ì²˜ë¦¬ ë¶ˆê°€ - ì„¸ë§ˆí¬ì–´ë¡œ ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
        self._generation_semaphore = asyncio.Semaphore(1)

    async def load_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸°)"""
        async with self._load_lock:
            # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ë©´ ìŠ¤í‚µ
            if model_name in self._models:
                self._current_model = model_name
                return True

            # ëª¨ë¸ ê²½ë¡œ í™•ì¸
            model_path = get_model_path(model_name)
            if not model_path:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” MLX ëª¨ë¸: {model_name}")
                return False

            print(f"ğŸ”„ MLX ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_name} ({model_path})")

            try:
                # ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
                if self._current_model and self._current_model != model_name:
                    print(f"ğŸ”„ ì´ì „ ëª¨ë¸ ì–¸ë¡œë“œ: {self._current_model}")
                    if self._current_model in self._models:
                        del self._models[self._current_model]
                        del self._tokenizers[self._current_model]

                # MLX ëª¨ë¸ ë¡œë“œ (ë™ê¸° ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
                loop = asyncio.get_event_loop()
                model, tokenizer = await loop.run_in_executor(
                    None, self._load_model_sync, model_path
                )
                self._models[model_name] = model
                self._tokenizers[model_name] = tokenizer
                self._current_model = model_name
                print(f"âœ… MLX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name} ({model_path})")
                return True
            except Exception as e:
                print(f"âŒ MLX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                return False

    def _load_model_sync(self, model_path: str):
        """ëª¨ë¸ ë™ê¸° ë¡œë“œ"""
        from mlx_lm import load
        return load(model_path)

    def is_loaded(self, model_name: Optional[str] = None) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        if model_name:
            return model_name in self._models
        return self._current_model is not None

    async def generate(
        self,
        messages: List[Dict[str, str]],
        model: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> Union[AsyncGenerator[str, None], str]:
        """í…ìŠ¤íŠ¸ ìƒì„± (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ì œí•œ)"""
        # ëª¨ë¸ ë¡œë“œ í™•ì¸ (ì„¸ë§ˆí¬ì–´ ì™¸ë¶€ì—ì„œ)
        if model not in self._models:
            await self.load_model(model)

        if model not in self._models:
            raise Exception(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model}")

        # ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        prompt = self._format_messages(messages, model)

        max_tokens = max_tokens or self.config.max_tokens
        temperature = temperature or self.config.temperature

        if stream:
            # ìŠ¤íŠ¸ë¦¬ë°: ì„¸ë§ˆí¬ì–´ê°€ í¬í•¨ëœ ë˜í¼ ì‚¬ìš©
            return self._generate_stream_with_semaphore(model, prompt, max_tokens, temperature)
        else:
            # ë¹„ìŠ¤íŠ¸ë¦¬ë°: ì„¸ë§ˆí¬ì–´ íšë“ í›„ ìƒì„±
            async with self._generation_semaphore:
                return await self._generate_full(model, prompt, max_tokens, temperature)

    async def _generate_stream_with_semaphore(
        self,
        model: str,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> AsyncGenerator[str, None]:
        """ì„¸ë§ˆí¬ì–´ê°€ í¬í•¨ëœ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        async with self._generation_semaphore:
            async for token in self._generate_stream(model, prompt, max_tokens, temperature):
                yield token

    def _format_messages(self, messages: List[Dict[str, str]], model: str) -> str:
        """ChatML í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ë³€í™˜"""
        formatted = ""
        is_qwen3 = model.startswith("mlx-qwen3")

        for i, msg in enumerate(messages):
            role = msg.get("role", "user")
            content = msg.get("content", "")

            # Qwen3: ë§ˆì§€ë§‰ user ë©”ì‹œì§€ì— /nothink ì¶”ê°€ (thinking ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ)
            if is_qwen3 and role == "user" and i == len(messages) - 1:
                if "/think" not in content and "/nothink" not in content:
                    content = content + " /nothink"

            if role == "system":
                formatted += f"<|im_start|>system\n{content}<|im_end|>\n"
            elif role == "user":
                formatted += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                formatted += f"<|im_start|>assistant\n{content}<|im_end|>\n"

        formatted += "<|im_start|>assistant\n"
        return formatted

    async def _generate_full(
        self,
        model: str,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """ì „ì²´ ì‘ë‹µ ìƒì„±"""
        loop = asyncio.get_event_loop()
        mlx_model = self._models[model]
        tokenizer = self._tokenizers[model]

        def _generate():
            from mlx_lm import generate
            return generate(
                mlx_model,
                tokenizer,
                prompt=prompt,
                max_tokens=max_tokens,
            )

        result = await loop.run_in_executor(None, _generate)
        # Qwen3 think íƒœê·¸ ì œê±°
        if model.startswith("mlx-qwen3"):
            result = remove_think_tags(result)
        return result

    async def _generate_stream(
        self,
        model: str,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        from mlx_lm import stream_generate
        import threading
        from queue import Queue as ThreadQueue

        mlx_model = self._models[model]
        tokenizer = self._tokenizers[model]

        # ìŠ¤ë ˆë“œ ì•ˆì „í•œ í ì‚¬ìš©
        token_queue = ThreadQueue()
        generation_done = threading.Event()

        def _stream_worker():
            try:
                for response in stream_generate(
                    mlx_model,
                    tokenizer,
                    prompt=prompt,
                    max_tokens=max_tokens,
                ):
                    # GenerationResponse ê°ì²´ì—ì„œ text ì¶”ì¶œ
                    token_text = response.text if hasattr(response, 'text') else str(response)
                    token_queue.put(token_text)
            except Exception as e:
                token_queue.put(Exception(str(e)))
            finally:
                generation_done.set()

        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        thread = threading.Thread(target=_stream_worker)
        thread.start()

        # í† í° ìŠ¤íŠ¸ë¦¬ë° (Qwen3 think íƒœê·¸ ì‹¤ì‹œê°„ í•„í„°ë§)
        is_qwen3 = model.startswith("mlx-qwen3")
        buffer = ""  # think íƒœê·¸ ê°ì§€ìš© ë²„í¼
        in_think = False  # think ë¸”ë¡ ë‚´ë¶€ ì—¬ë¶€
        passed_think = False  # think ë¸”ë¡ í†µê³¼ ì—¬ë¶€
        first_output = True  # ì²« ì¶œë ¥ ì—¬ë¶€ (ì•ìª½ ê³µë°± ì œê±°ìš©)

        while not generation_done.is_set() or not token_queue.empty():
            # íì—ì„œ í† í° ê°€ì ¸ì˜¤ê¸°
            while not token_queue.empty():
                token = token_queue.get_nowait()
                if isinstance(token, Exception):
                    raise token

                if not is_qwen3:
                    # Qwen3ê°€ ì•„ë‹ˆë©´ ë°”ë¡œ ì¶œë ¥
                    yield token
                    continue

                buffer += token

                # think ë¸”ë¡ ì²˜ë¦¬
                if not passed_think:
                    # <think> ì‹œì‘ ê°ì§€
                    if "<think>" in buffer:
                        in_think = True
                        buffer = buffer.split("<think>", 1)[1]

                    # </think> ì¢…ë£Œ ê°ì§€
                    if "</think>" in buffer and in_think:
                        in_think = False
                        passed_think = True
                        # </think> ì´í›„ ë‚´ìš©ë§Œ ìœ ì§€, ì•ë’¤ ê³µë°± ì œê±°
                        after_think = buffer.split("</think>", 1)[1]
                        buffer = after_think.lstrip("\n\r ")
                        continue

                    # think ë¸”ë¡ ë‚´ë¶€ë©´ ë²„í¼ë§Œ ìœ ì§€ (ì¶œë ¥ ì•ˆí•¨)
                    if in_think:
                        continue

                    # <ê°€ ìˆìœ¼ë©´ íƒœê·¸ ì‹œì‘ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ëŒ€ê¸°
                    if "<" in buffer:
                        continue

                # think ë¸”ë¡ í†µê³¼ í›„ ë˜ëŠ” think ì—†ëŠ” ê²½ìš°: ë°”ë¡œ ì¶œë ¥
                if buffer:
                    # ì²« ì¶œë ¥ì¼ ë•Œ ì•ìª½ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°
                    if first_output:
                        buffer = buffer.lstrip("\n\r ")
                        if buffer:
                            first_output = False
                            yield buffer
                            buffer = ""
                    else:
                        yield buffer
                        buffer = ""

            # ì§§ì€ ëŒ€ê¸° (CPU ì‚¬ìš©ëŸ‰ ì ˆì•½)
            if not generation_done.is_set():
                await asyncio.sleep(0.005)

        thread.join()

        # ë‚¨ì€ ë²„í¼ ì¶œë ¥
        if buffer:
            cleaned = remove_think_tags(buffer).strip()
            if cleaned:
                yield cleaned

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = "mlx-qwen2.5-14b",
        max_tokens: int = 4096,
        temperature: float = 0.7,
        stream: bool = False,
    ) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:
        """OpenAI í˜¸í™˜ Chat Completion API"""
        start_time = time.time()

        if stream:
            return self._stream_chat_completion(
                messages, model, max_tokens, temperature, start_time
            )

        # ì „ì²´ ì‘ë‹µ ìƒì„±
        content = await self.generate(
            messages=messages,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=False,
        )

        # í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )
        prompt_tokens = sum(len(m.get("content", "")) for m in messages) // 4
        completion_tokens = len(content) // 4

        return {
            "id": f"chatcmpl-mlx-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content,
                },
                "finish_reason": "stop",
            }],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
            },
        }

    async def _stream_chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str,
        max_tokens: int,
        temperature: float,
        start_time: float,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ìŠ¤íŠ¸ë¦¬ë° Chat Completion"""
        chat_id = f"chatcmpl-mlx-{int(time.time())}"

        async for token in await self.generate(
            messages=messages,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=True,
        ):
            yield {
                "id": chat_id,
                "object": "chat.completion.chunk",
                "created": int(start_time),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {
                        "content": token,
                    },
                    "finish_reason": None,
                }],
            }

        # ì¢…ë£Œ ì²­í¬
        yield {
            "id": chat_id,
            "object": "chat.completion.chunk",
            "created": int(start_time),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop",
            }],
        }

    def get_status(self) -> Dict[str, Any]:
        """MLX í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ"""
        return {
            "current_model": self._current_model,
            "loaded_models": list(self._models.keys()),
            "available_models": list(MLX_MODEL_MAP.keys()),
            "backend": "MLX (Apple Silicon)",
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_mlx_client: Optional[MLXClient] = None


def get_mlx_client() -> MLXClient:
    """MLX í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _mlx_client
    if _mlx_client is None:
        _mlx_client = MLXClient()
    return _mlx_client


async def init_mlx_client(config: Optional[MLXConfig] = None) -> MLXClient:
    """MLX í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ"""
    global _mlx_client
    _mlx_client = MLXClient(config)
    # ê¸°ë³¸ ëª¨ë¸ í”„ë¦¬ë¡œë“œ
    await _mlx_client.load_model(_mlx_client.config.default_model)
    return _mlx_client
