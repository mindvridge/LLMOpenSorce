"""í•œêµ­ì–´ ìµœì í™” ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸"""

import time
from typing import List, Optional, Tuple
from functools import lru_cache
from collections import OrderedDict


class EmbeddingClient:
    """SentenceTransformer ê¸°ë°˜ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (ìºì‹± ìµœì í™”)"""

    MODEL_NAME = "jhgan/ko-sroberta-multitask"
    DIMENSION = 768
    CACHE_SIZE = 1000  # ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜
    CACHE_TTL = 3600   # ìºì‹œ ìœ íš¨ ì‹œê°„ (1ì‹œê°„)

    def __init__(self, model_name: Optional[str] = None):
        self.model_name = model_name or self.MODEL_NAME
        self._model = None
        # ì¿¼ë¦¬ ìºì‹œ: {query: (embedding_tuple, timestamp)}
        self._query_cache: OrderedDict = OrderedDict()
        self._cache_hits = 0
        self._cache_misses = 0

    @property
    def model(self):
        """ëª¨ë¸ ì§€ì—° ë¡œë”©"""
        if self._model is None:
            from sentence_transformers import SentenceTransformer
            print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            self._model = SentenceTransformer(self.model_name)
            print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        return self._model

    def _get_from_cache(self, query: str) -> Optional[List[float]]:
        """ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ"""
        if query in self._query_cache:
            embedding_tuple, timestamp = self._query_cache[query]
            # TTL í™•ì¸
            if time.time() - timestamp < self.CACHE_TTL:
                # LRU: ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™
                self._query_cache.move_to_end(query)
                self._cache_hits += 1
                return list(embedding_tuple)
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self._query_cache[query]
        return None

    def _add_to_cache(self, query: str, embedding: List[float]):
        """ìºì‹œì— ì„ë² ë”© ì¶”ê°€"""
        # ìºì‹œ í¬ê¸° ì œí•œ
        while len(self._query_cache) >= self.CACHE_SIZE:
            self._query_cache.popitem(last=False)  # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ
        # íŠœí”Œë¡œ ì €ì¥ (ë¶ˆë³€ì„±)
        self._query_cache[query] = (tuple(embedding), time.time())

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ê° ë²¡í„°ëŠ” 768ì°¨ì›)
        """
        if not texts:
            return []
        embeddings = self.model.encode(texts, show_progress_bar=False)
        return embeddings.tolist()

    def embed_query(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ ì„ë² ë”© (ìºì‹± ì ìš©)

        ë™ì¼ ì¿¼ë¦¬ ì¬ìš”ì²­ ì‹œ ìºì‹œì—ì„œ ë°˜í™˜ (200-300ms ì ˆì•½)

        Args:
            query: ì„ë² ë”©í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (768ì°¨ì›)
        """
        # ìºì‹œ í™•ì¸
        cached = self._get_from_cache(query)
        if cached is not None:
            return cached

        # ìºì‹œ ë¯¸ìŠ¤: ì‹¤ì œ ì„ë² ë”© ê³„ì‚°
        self._cache_misses += 1
        embedding = self.model.encode(query, show_progress_bar=False)
        embedding_list = embedding.tolist()

        # ìºì‹œì— ì €ì¥
        self._add_to_cache(query, embedding_list)
        return embedding_list

    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total = self._cache_hits + self._cache_misses
        hit_rate = (self._cache_hits / total * 100) if total > 0 else 0
        return {
            "cache_size": len(self._query_cache),
            "max_size": self.CACHE_SIZE,
            "hits": self._cache_hits,
            "misses": self._cache_misses,
            "hit_rate": f"{hit_rate:.1f}%"
        }

    def get_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.DIMENSION


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_embedding_client: Optional[EmbeddingClient] = None


def get_embedding_client() -> EmbeddingClient:
    """ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _embedding_client
    if _embedding_client is None:
        _embedding_client = EmbeddingClient()
    return _embedding_client
