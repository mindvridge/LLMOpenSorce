"""í•œêµ­ì–´ ìµœì í™” ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (GPU ê°€ì† + ë°°ì¹˜ ì²˜ë¦¬ + ìºì‹±)"""

import time
import asyncio
import hashlib
from typing import List, Optional, Tuple, Dict
from functools import lru_cache
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
import threading


class EmbeddingClient:
    """SentenceTransformer ê¸°ë°˜ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (ìµœì í™” ë²„ì „)

    ìµœì í™” ì ìš©:
    1. ìºì‹± ê°•í™”: TTL 2ì‹œê°„, ìºì‹œ í¬ê¸° 2000
    2. ë°°ì¹˜ ì²˜ë¦¬: ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ëª¨ì•„ì„œ í•œë²ˆì— ì²˜ë¦¬
    3. GPU ê°€ì†: MLX ë°±ì—”ë“œ ì‚¬ìš© (Apple Silicon)
    """

    MODEL_NAME = "jhgan/ko-sroberta-multitask"
    DIMENSION = 768

    # ===== ìºì‹± ê°•í™” =====
    CACHE_SIZE = 2000      # 1000 â†’ 2000 (2ë°° ì¦ê°€)
    CACHE_TTL = 7200       # 1ì‹œê°„ â†’ 2ì‹œê°„ (ìºì‹œ ì¬ì‚¬ìš© ì¦ê°€)

    # ===== ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì • =====
    BATCH_SIZE = 16        # í•œë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜
    BATCH_TIMEOUT = 0.05   # ë°°ì¹˜ ìˆ˜ì§‘ ëŒ€ê¸° ì‹œê°„ (50ms)

    def __init__(self, model_name: Optional[str] = None, use_gpu: bool = True):
        self.model_name = model_name or self.MODEL_NAME
        self._model = None
        self._use_gpu = use_gpu
        self._device = None

        # ì¿¼ë¦¬ ìºì‹œ: {query_hash: (embedding_tuple, timestamp)}
        self._query_cache: OrderedDict = OrderedDict()
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache_lock = threading.Lock()

        # ë°°ì¹˜ ì²˜ë¦¬ìš©
        self._batch_queue: List[Tuple[str, asyncio.Future]] = []
        self._batch_lock = threading.Lock()
        self._batch_executor = ThreadPoolExecutor(max_workers=2)
        self._batch_processing = False

    @property
    def model(self):
        """ëª¨ë¸ ì§€ì—° ë¡œë”© (GPU ê°€ì† ì‹œë„)"""
        if self._model is None:
            from sentence_transformers import SentenceTransformer
            import torch

            print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")

            # GPU ê°€ì† ì„¤ì • (Apple Silicon MPS)
            if self._use_gpu:
                if torch.backends.mps.is_available():
                    self._device = "mps"
                    print("âš¡ Apple Silicon GPU (MPS) ê°€ì† í™œì„±í™”")
                elif torch.cuda.is_available():
                    self._device = "cuda"
                    print("âš¡ NVIDIA GPU (CUDA) ê°€ì† í™œì„±í™”")
                else:
                    self._device = "cpu"
                    print("ğŸ’» CPU ëª¨ë“œ (GPU ë¯¸ì‚¬ìš©)")
            else:
                self._device = "cpu"
                print("ğŸ’» CPU ëª¨ë“œ (ëª…ì‹œì  ë¹„í™œì„±í™”)")

            self._model = SentenceTransformer(self.model_name, device=self._device)
            print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ (device: {self._device})")

        return self._model

    def _get_cache_key(self, query: str) -> str:
        """ì¿¼ë¦¬ì˜ ìºì‹œ í‚¤ ìƒì„± (í•´ì‹œ ê¸°ë°˜)"""
        return hashlib.md5(query.encode()).hexdigest()

    def _get_from_cache(self, query: str) -> Optional[List[float]]:
        """ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        cache_key = self._get_cache_key(query)

        with self._cache_lock:
            if cache_key in self._query_cache:
                embedding_tuple, timestamp = self._query_cache[cache_key]
                # TTL í™•ì¸
                if time.time() - timestamp < self.CACHE_TTL:
                    # LRU: ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™
                    self._query_cache.move_to_end(cache_key)
                    self._cache_hits += 1
                    return list(embedding_tuple)
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                    del self._query_cache[cache_key]
        return None

    def _add_to_cache(self, query: str, embedding: List[float]):
        """ìºì‹œì— ì„ë² ë”© ì¶”ê°€ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        cache_key = self._get_cache_key(query)

        with self._cache_lock:
            # ìºì‹œ í¬ê¸° ì œí•œ
            while len(self._query_cache) >= self.CACHE_SIZE:
                self._query_cache.popitem(last=False)  # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ
            # íŠœí”Œë¡œ ì €ì¥ (ë¶ˆë³€ì„±)
            self._query_cache[cache_key] = (tuple(embedding), time.time())

    def _add_batch_to_cache(self, queries: List[str], embeddings: List[List[float]]):
        """ì—¬ëŸ¬ ì„ë² ë”©ì„ í•œë²ˆì— ìºì‹œì— ì¶”ê°€"""
        with self._cache_lock:
            for query, embedding in zip(queries, embeddings):
                cache_key = self._get_cache_key(query)
                while len(self._query_cache) >= self.CACHE_SIZE:
                    self._query_cache.popitem(last=False)
                self._query_cache[cache_key] = (tuple(embedding), time.time())

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ê° ë²¡í„°ëŠ” 768ì°¨ì›)
        """
        if not texts:
            return []

        # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        all_embeddings = []
        for i in range(0, len(texts), self.BATCH_SIZE):
            batch = texts[i:i + self.BATCH_SIZE]
            embeddings = self.model.encode(
                batch,
                show_progress_bar=False,
                batch_size=self.BATCH_SIZE,
                normalize_embeddings=True  # ì •ê·œí™”ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
            )
            all_embeddings.extend(embeddings.tolist())

        return all_embeddings

    def embed_query(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ ì„ë² ë”© (ìºì‹± ì ìš©)

        ë™ì¼ ì¿¼ë¦¬ ì¬ìš”ì²­ ì‹œ ìºì‹œì—ì„œ ë°˜í™˜ (200-300ms ì ˆì•½)

        Args:
            query: ì„ë² ë”©í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (768ì°¨ì›)
        """
        # ìºì‹œ í™•ì¸
        cached = self._get_from_cache(query)
        if cached is not None:
            return cached

        # ìºì‹œ ë¯¸ìŠ¤: ì‹¤ì œ ì„ë² ë”© ê³„ì‚°
        self._cache_misses += 1
        embedding = self.model.encode(
            query,
            show_progress_bar=False,
            normalize_embeddings=True
        )
        embedding_list = embedding.tolist()

        # ìºì‹œì— ì €ì¥
        self._add_to_cache(query, embedding_list)
        return embedding_list

    def embed_queries_batch(self, queries: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ ì¿¼ë¦¬ ë°°ì¹˜ ì„ë² ë”© (ìµœì í™”)

        ìºì‹œ íˆíŠ¸ì™€ ë¯¸ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬

        Args:
            queries: ì„ë² ë”©í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not queries:
            return []

        results = [None] * len(queries)
        uncached_indices = []
        uncached_queries = []

        # 1. ìºì‹œì—ì„œ ë¨¼ì € ì¡°íšŒ
        for i, query in enumerate(queries):
            cached = self._get_from_cache(query)
            if cached is not None:
                results[i] = cached
            else:
                uncached_indices.append(i)
                uncached_queries.append(query)

        # 2. ìºì‹œ ë¯¸ìŠ¤ëœ ì¿¼ë¦¬ë§Œ ë°°ì¹˜ ì²˜ë¦¬
        if uncached_queries:
            self._cache_misses += len(uncached_queries)
            embeddings = self.model.encode(
                uncached_queries,
                show_progress_bar=False,
                batch_size=self.BATCH_SIZE,
                normalize_embeddings=True
            )

            # ê²°ê³¼ ì €ì¥ ë° ìºì‹œ ì¶”ê°€
            embedding_lists = embeddings.tolist()
            self._add_batch_to_cache(uncached_queries, embedding_lists)

            for idx, embedding in zip(uncached_indices, embedding_lists):
                results[idx] = embedding

        return results

    async def embed_query_async(self, query: str) -> List[float]:
        """ë¹„ë™ê¸° ì¿¼ë¦¬ ì„ë² ë”©

        asyncio ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë¸”ë¡œí‚¹ ì—†ì´ ì„ë² ë”© ì‹¤í–‰
        """
        # ìºì‹œ í™•ì¸ (ë¹ ë¦„)
        cached = self._get_from_cache(query)
        if cached is not None:
            return cached

        # ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰ (ë¹„ë¸”ë¡œí‚¹)
        loop = asyncio.get_event_loop()
        embedding = await loop.run_in_executor(
            self._batch_executor,
            self.embed_query,
            query
        )
        return embedding

    async def embed_queries_batch_async(self, queries: List[str]) -> List[List[float]]:
        """ë¹„ë™ê¸° ë°°ì¹˜ ì„ë² ë”©

        ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ë¹„ë™ê¸°ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        """
        if not queries:
            return []

        loop = asyncio.get_event_loop()
        embeddings = await loop.run_in_executor(
            self._batch_executor,
            self.embed_queries_batch,
            queries
        )
        return embeddings

    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total = self._cache_hits + self._cache_misses
        hit_rate = (self._cache_hits / total * 100) if total > 0 else 0
        return {
            "cache_size": len(self._query_cache),
            "max_size": self.CACHE_SIZE,
            "hits": self._cache_hits,
            "misses": self._cache_misses,
            "hit_rate": f"{hit_rate:.1f}%",
            "device": self._device or "not_loaded",
            "ttl_seconds": self.CACHE_TTL
        }

    def get_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.DIMENSION

    def warmup(self):
        """ëª¨ë¸ ì›œì—… (ì²« ìš”ì²­ ì§€ì—° ë°©ì§€)"""
        _ = self.model  # ëª¨ë¸ ë¡œë”© íŠ¸ë¦¬ê±°
        # ë”ë¯¸ ì„ë² ë”©ìœ¼ë¡œ GPU ì›œì—…
        _ = self.embed_query("ì›œì—… í…ŒìŠ¤íŠ¸")
        print(f"âœ… ì„ë² ë”© ëª¨ë¸ ì›œì—… ì™„ë£Œ (device: {self._device})")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_embedding_client: Optional[EmbeddingClient] = None


def get_embedding_client(use_gpu: bool = True) -> EmbeddingClient:
    """ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜

    Args:
        use_gpu: GPU ê°€ì† ì‚¬ìš© ì—¬ë¶€ (Apple Silicon MPS ë˜ëŠ” CUDA)
    """
    global _embedding_client
    if _embedding_client is None:
        _embedding_client = EmbeddingClient(use_gpu=use_gpu)
    return _embedding_client
